{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "삶의 전반에 걸쳐 상호작용(interation)은 환경과 우리 스스로에 대해 배우게 하는 근원적인 요소이다. 어떤 행위(action)를 하는 과정에서 환경과의 상호작용으로 얻어지는 정보(행위와 그에 대한 결과)를 바탕으로 우리는 어떻게하면 목표를 성취할 수 있을지에 대해 점차 깨우치게 된다. 이렇게 상호작용을 통해 배우는 것은 대부분의 learning and intelligence의 기저에 있는 발상일 것이다. \n",
    "이 책에서는 상호작용을 통한 학습에 대해 계산적인 접근(computational approach)을 해보도록 한다. 그리고 그 여러가지 기법들 중에서도 *reinforcement learning*이라는, 다른 기계학습의 기법들 보다 좀 더, 목표지향적 상호작용 학습(goal-directed learning from interation)에 중점을 두는 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning은 어떤 상황(situation)에 대응되는 보상(reward)을 최대화 할 수 있는 행동(action)을 학습하는 것이다. 학습의 주체는 상황에 따른 가장 유리한 action이 무엇인지에 대해 사전에 알고있지 못하므로 이를 찾는 과정에서 수많은 시행착오를 겪게될 것이다. 또한 좀 더 복잡한 상황에서는 현재의 action이 미래의 순차적인 보상에 영향을 줄 수도 있다. Reinforcement learning에서는 이 두 가지 사항-trial-and-error search and delayed reward-을 가장 중요한 특성으로 뽑을 수 있다.\n",
    "\n",
    "Reinforcement learning은 다음과 같은 세 가지 큰 주제로 나누어 볼 수 있다: 1. 문제(problem), 2. 어떤 문제에 대해 잘 동작하는 solution method들의 class, 3. 이런 문제와 solution method들을 연구하는 field. Reinforcement learning의 문제는 incompletely-known Markov decision processes(MDP)로 형식화(formalize)된다. 여기서 MDP는 sensation, action, goal의 세 가지 개념을 포함하는데, 학습중인 agent는 환경에 대한 어떤 상태(state)에 대해 인지할 수 있어야하고, 상태에 어떤 영향을 주는 action을 선택할 수 있어야하며, 또한 어떤 목표(goal)를 가지고있어야 한다.\n",
    "\n",
    "Reinforcement learning은 *supervised learning*과는 다른 범주에 속한다. Supervised learning에서는 어떤 도메인에 대한 배경지식이 있는 전문가로부터 training set을 제공받게 되고, 이 training set을 통해 학습함으로써 training set에 포함되지 않은 입력에 대해서도 적절한 출력을 보이게끔 기대한다. 하지만 reinforcement learning의 문제에서는 agent가 마주칠 법한 상황과 그에 대한 행동에 대해 정확하고 적절한 예시를 얻어내는 것이 쉽지 않은 경우가 많다. 또한 reinforcement learning은 *unsupervised learning*과도 다른 범주에 속한다. Unsupervised learning은 unlabelled data로부터 hidden structure를 찾는 것을 목표로 하는 것에 반해, reinforcement learning은 보상을 최대화시키는 것이 주요한 목표이기 때문이다. 그러므로 reinforcement learning은 기계학습의 세 번째 패러다임으로 간주하도록 한다.\n",
    "\n",
    "Reinforcement learning에서의 커다란 과제중 하나는 exploration과 exploitation 사이에서의 적절한 trade-off를 하는 것이다. 더 높은 보상을 받기 위해서는 주어진 상황에서의 더 적절한 action을 선택(exploit)해야 하는데, 이때 각 action들의 가치에 대해 알기 위해서는 사전에 탐험(explore)을 하는 것이 필요하다. 특히 stochastic task에서는 expected reward에 대해 신뢰할 수 있는 추정을 하기 위해서 각 action들이 더욱 많이 선택되어야 하는데, 이렇게 탐험을 하기 위해서는 지금 당장 최선이라고 믿어지는 action을 포기할 수 있어야한다. 우리는 이러한 exploration과 exploitation의 관계를 *exploration-exploitation dilemma*라고 부른다. 이는 reinforcement learning을 supervised learning, unsupervised learning과 구분짓는 또 한가지의 차이점이다.\n",
    "\n",
    "흥미로운 점은 modern reinforcement learning이 다른 공학 및 과학 분야들과 섞여 유의미한 결과물을 내고있다는 것이다. 가령, parameterized approximator를 이용한 reinforcement learning의 방법론들은 operations research and control theroy에서의 \"[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\"를 해결하는데 이용되기도 한다. 또한 reinforcement learning의 많은 핵심 알고리즘들이 사람이나 동물들이 학습하는 과정과도 굉장히 유사하기 때문에 심리학 및 뇌과학 분야에서 활발히 이용되는 경우도 있다.\n",
    "\n",
    "1960년 말 이후로 많은 AI 연구자들은 general principle에 대한 연구 대신 어떤 도메인에서의 특수한 문제들을 해결하기 위한 방법론을 찾는 방향으로 노선을 변경하였다. 그렇게 얻어낸 작은 기능들이 엄청나게 많이 모이게되면 그것은 결국 지능을 갖춘것과 동일한 것이지 않을까라는 가정에서다. 반면 최근의 인공지능 학계에서는 학습에 대한 general principles를 찾기위한 연구가 더욱 활발히 이루어지고 있고, reinforcement learning 또한 그러한 방향을 함께하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
