{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "삶의 전반에 걸쳐 상호작용(interation)은 환경과 우리 스스로에 대해 배우게 하는 근원적인 요소이다. 어떤 행위(action)를 하는 과정에서 환경과의 상호작용으로 얻어지는 정보(행위와 그에 대한 결과)를 바탕으로 우리는 어떻게하면 목표를 성취할 수 있을지에 대해 점차 깨우치게 된다. 이렇게 상호작용을 통해 배우는 것은 대부분의 learning and intelligence의 기저에 있는 발상일 것이다. \n",
    "이 책에서는 상호작용을 통한 학습에 대해 계산적인 접근(computational approach)을 해보도록 한다. 그리고 그 여러가지 기법들 중에서도 *reinforcement learning*이라는, 다른 기계학습의 기법들 보다 좀 더, 목표지향적 상호작용 학습(goal-directed learning from interation)에 중점을 두는 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning은 어떤 상황(situation)에 대응되는 보상(reward)을 최대화 할 수 있는 행동(action)에 대해 학습하는 것이다. 학습의 주체는 상황에 따른 가장 유리한 action이 무엇인지를 사전에 알고있지 못하므로 이를 찾는 과정에서 수많은 시행착오를 겪게될 것이다. 또한 좀 더 복잡한 상황에서는 현재의 action이 미래의 순차적인 보상에 영향을 줄 수도 있다. Reinforcement learning에서는 이 두 가지 사항-trial-and-error search and delayed reward-을 가장 중요한 특성으로 뽑을 수 있다.\n",
    "\n",
    "Reinforcement learning은 다음과 같은 세 가지 큰 주제로 나누어 볼 수 있다: 1. 문제(problem), 2. 어떤 문제에 대해 잘 동작하는 solution method들의 class, 3. 이런 문제와 solution method들을 연구하는 field. Reinforcement learning의 문제는 incompletely-known Markov decision processes(MDP)로 형식화(formalize)된다. 여기서 MDP는 sensation, action, goal의 세 가지 개념을 포함하는데, 학습중인 agent는 환경에 대한 어떤 상태(state)에 대해 인지할 수 있어야하고, 상태에 어떤 영향을 주는 action을 선택할 수 있어야하며, 또한 어떤 목표(goal)를 가지고있어야 한다.\n",
    "\n",
    "Reinforcement learning은 *supervised learning*과는 다른 범주에 속한다. Supervised learning에서는 어떤 도메인의 전문가로부터 training set을 제공받게 되고, 이 training set을 통해 학습함으로써 training set에 포함되지 않은 입력에 대해서도 적절한 출력을 보이게끔 기대한다. 하지만 reinforcement learning의 문제에서는 agent가 마주칠 법한 상황과 그에 대한 행동에 대해 정확하고 적절한 예시를 얻어내는 것이 쉽지 않은 경우가 많다. Reinforcement learning은 또한 *unsupervised learning*과도 다른 범주에 속한다. Unsupervised learning은 unlabelled data로부터 hidden structure를 찾는 것을 목표로 하는 것에 반해, reinforcement learning은 보상을 최대화시키는 것이 주요한 목표이기 때문이다. 그러므로 reinforcement learning은 기계학습의 세 번째 패러다임으로 간주하도록 한다.\n",
    "\n",
    "Reinforcement learning에서의 커다란 과제중 하나는 exploration과 exploitation 사이에서의 적절한 trade-off를 하는 것이다. 더 높은 보상을 받기 위해서는 주어진 상황에서의 더 적절한 action을 선택(exploit)해야 하는데, 이때 각 action들의 가치에 대해 알기 위해서는 사전에 탐험(explore)을 하는 것이 필요하다. 특히 stochastic task에서는 expected reward에 대해 신뢰할 수 있는 추정을 하기 위해서 각 action들이 더욱 많이 선택되어야 하는데, 이렇게 탐험을 하기 위해서는 지금 당장 최선이라고 믿어지는 action을 포기할 수 있어야한다. 우리는 이러한 exploration과 exploitation의 관계를 *exploration-exploitation dilemma*라고 부른다. 이는 reinforcement learning을 supervised learning, unsupervised learning과 구분짓는 또 한가지의 차이점이다.\n",
    "\n",
    "흥미로운 점은 modern reinforcement learning이 다른 공학 및 과학 분야들과 섞여 유의미한 결과물을 내고있다는 것이다. 가령, parameterized approximator를 이용한 reinforcement learning의 방법론들은 operations research and control theroy에서의 \"[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\"를 해결하는데 이용되기도 한다. 또한 reinforcement learning의 많은 핵심 알고리즘들이 사람이나 동물들이 학습하는 과정과도 굉장히 유사하기 때문에 심리학 및 뇌과학 분야에서 활발히 이용되는 경우도 있다.\n",
    "\n",
    "1960년 말 이후로 많은 AI 연구자들은 general principle에 대한 연구 대신 어떤 도메인에서의 특정 문제들을 해결하는 방향으로 노선을 변경하였다. 그렇게 얻어낸 작은 기능들이 엄청나게 많이 모이게되면 그것이 결국 지능을 갖춘것과 동일한 것이리라는 가정에서다. 반면 최근의 인공지능 학계에서는 학습에 대한 general principles를 찾기위한 연구가 더욱 활발히 이루어지고 있고, reinforcement learning 또한 그러한 흐름을 함께하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적용가능한 예시나 응용사례들을 살펴보도록 하자.\n",
    "\n",
    "- 최고수 체스선수가 수를 두는 것. 각 위치와 말의 이동에 대한 즉각적이고 직관적인 판단 아래에 다음 수에 대한 판단을 내릴 수 있다.\n",
    "- 석유정제시설의 적응제어기가 실시간으로 파라미터를 조정하는 것. 제어기는 명시된 한계비용에 근거하여 산출량/비용/품질에 대한 적절한 균형점을 찾아야한다.\n",
    "- 새끼 가젤이 태어난 후 걷기위한 몸부림을 치는 것. 생후 30분만에 새끼 가젤은 시속 20마일로 뛸 수 있다.\n",
    "- 무선 청소로봇이 청소를 위해 방에 들어갈지 혹은 충전을 위해 충전소로 돌아갈지 결정하는 것. 현재 배터리의 잔량과 충전소를 얼마나 빨리 찾아갈 수 있는지에 대한 과거의 경험에 의거해 결정된다.\n",
    "\n",
    "위 예시들을 살펴보면 공통적으로 1. active decision making agent와 환경(environment) 사이의 상호작용(interaction)이 있고, 2. agent는 환경의 불확실성 속에서 목적(goal)을 달성하기 위해 동작하며, 3. agent의 행동(action)은 환경의 차후 상태(state)에 영향을 미침을 알 수 있다. 이때 행동에 의해 발생하는 결과는 완전히 예측될 수 없으므로 agent는 계속해서 환경을 관찰하고 적합한 반응을 해야한다. 또한 agent는 성능을 점차 향상시키기 위해 과거에 얻은 경험을 잘 활용할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent와 environment 외에도 reinforcement learning system을 구성하는 4가지의 하위요소들이 있다. 바로 policy, reward signal, value function, 그리고 (optionally) 환경에 대한 모델(model)이다.\n",
    "\n",
    "1. **policy:** policy는 현재의 상태(state)에 대해 어떤 행동(action)을 결정하는 역할을 한다. 간단하게는 lookup table이 될 수도 있고, 복잡하게는 상당한 계산비용을 필요로하는 탐색과정이 될 수도 있다. 보통 policy는 확률적(stochastic)인 경우가 많다.\n",
    "2. **reward signal:** Agent가 어떤 행동(action)을 할 때마다 환경은 agent에게 숫자 하나를 보내주는데, 이 것이 바로 reward signal이다. Reinforcement learning의 목표는 reward의 총합을 최대화시키는 것이기 때문에 reward signal에 대한 정의가 바로 목표에 대한 정의라고 봐도 무방하다. 또한 action을 선택하고 reward signal이 발생하면 그 신호의 내용에 따라 방금 선택했던 action의 가치가 변동될 것이므로, 즉 reward signal은 policy의 변경을 위한 우선적인 근거가 된다. 보통 reward signal은 state와 action에 대한 확률함수(stochastic function)이다.\n",
    "3. **value function:** reward signal이 지금 당장 얻게되는 보상이라고 한다면, value function은 좀 더 장기적인 관점에서의 가치를 의미한다. 대략적으로 말해서 state에 대한 value라고 하면, 그 state를 시작으로 agent가 얻게 되리라 예상되는 reward에 대한 총합이라고 할 수 있다. 즉, 어떤 state에서 지금 당장 받게되는 reward가 높지 않더라도, 뒤이어 만나게되는 state들이 높은 reward를 준다면 value는 높은 값을 갖게 될 것이다. Value는 이전에 발생한 reward들에 의해 계산되므로 reward가 없으면 value또 존재할 수 없다. 하지만 판단에 앞서 평가나 선택을 함에 있어서는 (가장 높은) value가 우선적으로 염두된다. Value를 어떻게 잘 산출해내는지에 대한 문제는 reinforcement learning에서 아주 중요하고도 어려운 주제에 속한다.\n",
    "4. **model:** model은 environment의 behavior를 추론하는 무언가이다. 예를들어, 어떤 state와 action이 주어졌을때 model은 (마치 환경이 그러하듯이) 그 결과로써 발생하는 다음 state와 다음 reward를 반환한다. Model은 planning을 위해 사용되는데, 여기서 planning이란 일련의 action들에 의해 발생할 수 있는 미래의 상황에 대해서 직접 경험해보기 이전에 고려해보는 방법을 칭한다. Reinforcement learning은 model과 planning을 이용하는 *model-based methods*와 그 반대의 개념인 (명시적인 시행착오에 의해 학습을 하는) *model-free methods*로 구분할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Limitations and Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
